{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement 1: Stochatic Environment\n",
    "**Pricing algorithm: Build a pricing strategy using the continuous set of prices p âˆˆ [0, 1] and Gaussian Processes.**\\\n",
    "Bidding algorithm: Consider a sequence of second-price auctions. Build two learning algorithms to deal with the bidding problem:\n",
    "- primal-dual algorithm for truthful auctions\n",
    "- UCB-like algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Pricing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticPricingEnvironment:\n",
    "    def __init__(self, cost, max_price=1.0, model='linear', alpha=None, beta=None):\n",
    "        self.cost = cost\n",
    "        self.max_price = max_price\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        # self.conversion_probability = StochasticPricingEnvironment.generate_probability(model, alpha, beta)\n",
    "        np.random.seed(42)\n",
    "\n",
    "    # given settled price and number customer\n",
    "    # returns a sample of number of sale and profit\n",
    "    def round(self, price_t, n_costumer_t):\n",
    "        # number of sale at time t\n",
    "        number_sale_t = np.random.binomial(n_costumer_t, StochasticPricingEnvironment.generate_probability(self.model, self.alpha, self.beta, price_t))\n",
    "        \n",
    "        # profit at time t\n",
    "        profit_t = (price_t - self.cost)*number_sale_t\n",
    "\n",
    "        return number_sale_t, profit_t\n",
    "\n",
    "    # return a probability function that \n",
    "    # takes as input the selling price\n",
    "    # return as output the probability of selling \n",
    "    @staticmethod\n",
    "    def generate_probability(model, a, b, price):\n",
    "        def isNotNone(s,d):\n",
    "            if s is None:\n",
    "                return d\n",
    "            else:\n",
    "                return s\n",
    "        \n",
    "        if model == 'linear':\n",
    "            alpha = isNotNone(a, 1) # Intercept\n",
    "            beta = isNotNone(b, -1)  # Slope\n",
    "            purchase_probabilities = alpha + beta * price\n",
    "            \n",
    "        elif model == 'logit':\n",
    "            alpha = isNotNone(a, 0)\n",
    "            beta = isNotNone(b, -5)\n",
    "            purchase_probabilities = 1 / (1 + np.exp(-(alpha + beta * price)))\n",
    "            \n",
    "        elif model == 'probit':\n",
    "            alpha = isNotNone(a, 0)\n",
    "            beta = isNotNone(b, -5)\n",
    "            purchase_probabilities = norm.cdf(alpha + beta * price)\n",
    "            \n",
    "        elif model == 'KERNEL':\n",
    "            purchase_probabilities = np.abs(np.sin(2 * np.pi * price) * np.exp(-price * 5) + price * .10) + .1\n",
    "\n",
    "        return purchase_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricing Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFGaussianProcess:\n",
    "    def __init__(self, scale=1, reg=1e-2):\n",
    "        self.scale = scale \n",
    "        self.reg = reg\n",
    "        self.k_xx_inv = None\n",
    "\n",
    "    def rbf_kernel_incr_inv(self, B, C, D):\n",
    "        temp = np.linalg.inv(D - C @ self.k_xx_inv @ B)\n",
    "        block1 = self.k_xx_inv + self.k_xx_inv @ B @ temp @ C @ self.k_xx_inv\n",
    "        block2 = - self.k_xx_inv @ B @ temp\n",
    "        block3 = - temp @ C @ self.k_xx_inv\n",
    "        block4 = temp\n",
    "        res1 = np.concatenate((block1, block2), axis=1)\n",
    "        res2 = np.concatenate((block3, block4), axis=1)\n",
    "        res = np.concatenate((res1, res2), axis=0)\n",
    "        return res\n",
    "\n",
    "    def rbf_kernel(self, a, b):\n",
    "        a_ = a.reshape(-1, 1)\n",
    "        b_ = b.reshape(-1, 1)\n",
    "        output = -1 * np.ones((a_.shape[0], b_.shape[0]))\n",
    "        for i in range(a_.shape[0]):\n",
    "            output[i, :] = np.power(a_[i] - b_, 2).ravel()\n",
    "        return np.exp(-self.scale * output)\n",
    "    \n",
    "    def fit(self, x=np.array([]), y=np.array([])):\n",
    "        x,y = np.array(x),np.array(y)\n",
    "        if self.k_xx_inv is None:\n",
    "            self.y = y.reshape(-1,1)\n",
    "            self.x = x.reshape(-1,1)\n",
    "            k_xx = self.rbf_kernel(self.x, self.x) + self.reg * np.eye(self.x.shape[0])\n",
    "            self.k_xx_inv = np.linalg.inv(k_xx)\n",
    "        else:\n",
    "            B = self.rbf_kernel(self.x, x)\n",
    "            self.x = np.vstack((self.x, x))\n",
    "            self.y = np.vstack((self.y, y))\n",
    "            self.k_xx_inv = self.rbf_kernel_incr_inv(B, B.T, np.array([1 + self.reg]))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_predict):\n",
    "        k = self.rbf_kernel(x_predict, self.x)\n",
    "\n",
    "        mu_hat = k @ self.k_xx_inv @ self.y\n",
    "        sigma_hat = 1 - np.diag(k @ self.k_xx_inv @ k.T)\n",
    "\n",
    "        return mu_hat.ravel(), sigma_hat.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Process Agent with RBF kernel and UCB algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the agent's point of view, action set is [0,1]. If the actual actions are outside this\n",
    "# set, we can always perform a rescaling outside the class.\n",
    "class GPUCBAgent:\n",
    "    def __init__(self, T, discretization=100):\n",
    "        self.T = T\n",
    "        self.arms = np.linspace(0, 1, discretization)\n",
    "        self.gp = RBFGaussianProcess(scale=2).fit()\n",
    "        self.a_t = None\n",
    "        self.action_hist = np.array([])\n",
    "        self.reward_hist = np.array([])\n",
    "        self.mu_t = np.zeros(discretization)\n",
    "        self.sigma_t = np.zeros(discretization)\n",
    "        self.gamma = lambda t: np.log(t+1)**2 \n",
    "        self.beta = lambda t: 1 + 0.5*np.sqrt(2 * (self.gamma(t) + 1 + np.log(T)))\n",
    "        self.N_pulls = np.zeros(discretization)\n",
    "        self.t = 0\n",
    "    \n",
    "    def pull_arm(self):\n",
    "        self.mu_t, self.sigma_t = self.gp.predict(self.arms) \n",
    "        #    averages of GP  +  estimation uncertainty\n",
    "        ucbs = self.mu_t + self.beta(self.t) * self.sigma_t\n",
    "        self.a_t = np.argmax(ucbs)\n",
    "        return self.arms[self.a_t]\n",
    "    \n",
    "    def update(self, profit_per_user_t):\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.action_hist = np.append(self.action_hist, self.arms[self.a_t])\n",
    "        self.reward_hist = np.append(self.reward_hist, profit_per_user_t)\n",
    "        self.gp = self.gp.fit(self.arms[self.a_t], profit_per_user_t)\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Price Auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondPriceAuction:\n",
    "    def __init__(self, ctrs):\n",
    "        # ctr = click though rate = lambda * q\n",
    "        self.ctrs = ctrs\n",
    "        self.n_adv = len(self.ctrs)\n",
    "    \n",
    "    def get_winners(self, bids):\n",
    "        # sort not by bids, but estimated values\n",
    "        adv_values = self.ctrs*bids\n",
    "        adv_ranking = np.argsort(adv_values)\n",
    "        winner = adv_ranking[-1]\n",
    "        return winner, adv_values\n",
    "    \n",
    "    def get_payments_per_click(self, winners, values, bids):\n",
    "        adv_ranking = np.argsort(values)\n",
    "        second = adv_ranking[-2]\n",
    "        payment = values[second]/self.ctrs[winners]\n",
    "        return payment.round(2)\n",
    "    \n",
    "    def round(self, bids):\n",
    "        # given bids, return winner(s) and the estimated values (product of 3 terms : lambda, q, bid) of winner(s)\n",
    "        winners, values = self.get_winners(bids) # allocation mechanism!\n",
    "        payments_per_click = self.get_payments_per_click(winners, values, bids)\n",
    "        return winners, payments_per_click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidding Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primal-dual algorithm -> Multiplicative Pacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativePacingAgent:\n",
    "    def __init__(self, valuation, budget, T, eta):\n",
    "        self.valuation = valuation\n",
    "        self.budget = budget\n",
    "        self.eta = eta # learning rate\n",
    "        self.T = T\n",
    "        self.rho = self.budget/self.T\n",
    "        self.lmbd = 1  # shouldn't this be initialized with 0?\n",
    "        self.t = 0\n",
    "\n",
    "    def bid(self):\n",
    "        if self.budget < 1:\n",
    "            return 0\n",
    "        return self.valuation/(self.lmbd+1)\n",
    "    \n",
    "    def update(self, f_t, c_t):\n",
    "        self.lmbd = np.clip(self.lmbd-self.eta*(self.rho-c_t), \n",
    "                            a_min=0, a_max=1/self.rho)\n",
    "        self.budget -= c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCB-like algorithm -> UCB-Multiplicative Pacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBMultiPaceAgent:\n",
    "    def __init__(self, valuation, budget, T, eta, number_rhos, range=1):\n",
    "        self.valuation = valuation\n",
    "        self.budget = budget\n",
    "        self.eta = eta # learning rate\n",
    "        self.T = T\n",
    "        self.range = range\n",
    "        self.used_rho = None\n",
    "        self.number_rhos = number_rhos\n",
    "        self.average_rewards = np.zeros(number_rhos)\n",
    "        self.N_pulls = np.zeros(number_rhos)\n",
    "        self.rhos = np.linspace(budget/T, valuation, number_rhos)\n",
    "        self.lmbds = np.ones(number_rhos)\n",
    "        self.t = 0\n",
    "\n",
    "    def bid(self):\n",
    "        # play every arm once\n",
    "        if self.t < self.number_rhos:\n",
    "            self.used_rho = self.t \n",
    "        else:\n",
    "            # compute UCB for every arms, choose arm with highest UCB\n",
    "            ucbs = self.average_rewards + self.range*np.sqrt(2*np.log(self.T)/self.N_pulls)\n",
    "            self.used_rho = np.argmax(ucbs)\n",
    "    \n",
    "        if self.budget < 1:\n",
    "            return 0\n",
    "        return self.valuation/(self.lmbds[self.used_rho]+1)\n",
    "    \n",
    "    def update(self, f_t, c_t):\n",
    "        self.N_pulls[self.used_rho] += 1\n",
    "        # update only used rho's average reward\n",
    "        self.average_rewards[self.used_rho] += (f_t - self.average_rewards[self.used_rho])/self.N_pulls[self.used_rho]\n",
    "        # update all rhos's lmbd according to c_t and their rho value\n",
    "        self.lmbds = np.clip(self.lmbds-self.eta*(self.rhos-c_t), \n",
    "                            a_min=0, a_max=1/self.rhos)\n",
    "        self.budget -= c_t\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCB-like algorithm -> simple UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    def __init__(self, valuation, budget, T, K=100, range=1):\n",
    "        self.T = T\n",
    "        self.valuation = valuation\n",
    "        self.budget = budget\n",
    "        self.K = K\n",
    "        self.range = range\n",
    "        self.arms = np.linspace(0, valuation, K)\n",
    "        self.average_rewards = np.zeros(K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.a_t = None\n",
    "        self.t = 0\n",
    "\n",
    "    def bid(self):\n",
    "        # if budget depleted\n",
    "        if self.budget < 1:\n",
    "            return 0\n",
    "        \n",
    "        # play every arm at least once\n",
    "        if self.t < self.K:\n",
    "            self.a_t = self.t\n",
    "        else:\n",
    "            # compute UCB for every arms, choose arm with highest UCB\n",
    "            ucbs = self.average_rewards + self.range*np.sqrt(2*np.log(self.T)/self.N_pulls)\n",
    "            self.a_t = np.argmax(ucbs)\n",
    "        return self.arms[self.a_t]\n",
    "\n",
    "    def update(self, f_t, c_t):\n",
    "        self.t += 1\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.average_rewards[self.a_t] += (f_t - self.average_rewards[self.a_t])/self.N_pulls[self.a_t]\n",
    "        self.budget -= c_t"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
